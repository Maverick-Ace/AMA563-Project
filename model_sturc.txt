gat_loss_112



class GAT_MODEL(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, edge_features_dim, drop_rate=0.5, inner_weights=None, outer_weights=None):
        super(GAT_MODEL, self).__init__()
        self.inner_weights = inner_weights
        self.outer_weights = outer_weights

        # Linear层
        self.fc1 = nn.Linear(hidden_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)

        # GAT层
        self.conv1 = GATConv(
            -1, 
            hidden_size, 
            heads=8, 
            dropout=drop_rate,
            concat=True,
            negative_slope=0.2,
            add_self_loops=True,
            edge_dim=edge_features_dim,
            bias=True,
            residual=True)
        
        self.conv2 = GATConv(
            hidden_size*8, 
            hidden_size, 
            heads=8, 
            dropout=drop_rate,
            concat=True,
            negative_slope=0.2,
            add_self_loops=True,
            edge_dim=edge_features_dim,
            bias=True,
            residual=True)

        self.conv_out = GATConv(
            hidden_size*8, 
            output_size, 
            heads=1, 
            dropout=0.2,
            concat=False,
            negative_slope=0.2,
            add_self_loops=True,
            edge_dim=edge_features_dim,
            bias=True,
            residual=True)
        
    def forward(self, x, edge_index, edge_attr, edge_weight, batch):
        # 去除特殊值填充
        maskx = (x != 1000).float()
        x = x * maskx
        maske = (edge_attr != 1000).float()
        edge_attr = edge_attr * maske

        # # 对edge_attr进行加权，每一个权重值乘以一行特征(pls r2作为权重计算指标)
        # edge_attr = edge_attr * edge_weight.view(-1, 1)

        # # 对x进行加权，元素对应相乘（pls模型的权重）
        # x = x.reshape(-1, 8, x.shape[1])
        # x = torch.cat([x[i, :, :] * self.inner_weights for i in range(x.shape[0])], dim=0)

        # # 对edge_attr进行加权，元素对应相乘（pls模型的权重）
        # edge_attr = edge_attr.reshape(-1, 56, edge_attr.shape[1])
        # edge_attr = torch.cat([edge_attr[i, :, :] * self.outer_weights for i in range(edge_attr.shape[0])], dim=0)

        x = F.relu(self.conv1(x, edge_index, edge_attr))
        x = F.relu(self.conv2(x, edge_index, edge_attr))
        x = self.conv_out(x, edge_index, edge_attr)
        
        # print(x.shape)
        # print(batch, batch.shape)
        # assert False
        x = x.reshape(-1, 8)
        # print(x.shape)
        # 沿第二个维度平均池化
        x = torch.mean(x, dim=1, keepdim=True)
        # print(x.shape)
        assert x.shape[1] == 1
        return x
    
    def loss(self, output, target):
        target = torch.tensor(np.array(target), dtype=torch.float32)
        # print(output.shape, target.shape)
        
        output = output.to('cuda')
        target = target.to('cuda')
        loss = F.mse_loss(output, target)
        # print(output, target)
        # print(loss)
        # assert False
        return loss
    
# 创建模型实例
model = GAT_MODEL(
    input_size=train_node_features.shape[2], 
    hidden_size=32, 
    output_size=1,
    edge_features_dim=train_edge_features.shape[2])

# 训练模型
best_model, train_loss_list, test_loss_list, r2_list, mse_list = train_gat(
    train_loader, test_loader, model, epochs=400, lr=0.001, weight_decay=1e-4, metrics=['mse', 'r2'], print_info=True, early_stop=40)

# 保存模型
save_model = input("Save model? (y/n): ")
if save_model.lower() == 'y':
    # 是否存在同名文件
    if os.path.exists('gat_model.pth'):
        # 提示
        print("Model file already exists.")
        # 是否覆盖
        overwrite = input("Overwrite? (y/n): ")
        if overwrite.lower() == 'y':
            model_save_path = os.path.join('.', 'gat_model.pth')
            torch.save(best_model, model_save_path)
    else:
        model_save_path = os.path.join('.', 'gat_model.pth')
        torch.save(best_model, model_save_path)

-------------------------------------------------

gat_loss_72

class GAT_MODEL(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, edge_features_dim, drop_rate=0.5, inner_weights=None, outer_weights=None):
        super(GAT_MODEL, self).__init__()
        self.inner_weights = inner_weights
        self.outer_weights = outer_weights

        # Linear层
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.output = nn.Linear(hidden_size*2*8, output_size)

        # GAT层
        self.conv1 = GATConv(
            -1, 
            hidden_size, 
            heads=8, 
            dropout=drop_rate,
            concat=True,
            negative_slope=0.2,
            add_self_loops=True,
            edge_dim=edge_features_dim,
            bias=True,
            residual=True)
        
        self.conv2 = GATConv(
            hidden_size*8, 
            hidden_size, 
            heads=8, 
            dropout=drop_rate,
            concat=True,
            negative_slope=0.2,
            add_self_loops=True,
            edge_dim=edge_features_dim,
            bias=True,
            residual=True)

        self.conv_out = GATConv(
            hidden_size*8, 
            hidden_size, 
            heads=1, 
            dropout=0.2,
            concat=False,
            negative_slope=0.2,
            add_self_loops=True,
            edge_dim=edge_features_dim,
            bias=True,
            residual=True)
        
    def forward(self, x, edge_index, edge_attr, edge_weight, batch):
        # 去除特殊值填充
        maskx = (x != 1000).float()
        x = x * maskx
        maske = (edge_attr != 1000).float()
        edge_attr = edge_attr * maske

        # # 对edge_attr进行加权，每一个权重值乘以一行特征(pls r2作为权重计算指标)
        # edge_attr = edge_attr * edge_weight.view(-1, 1)

        # # 对x进行加权，元素对应相乘（pls模型的权重）
        # x = x.reshape(-1, 8, x.shape[1])
        # x = torch.cat([x[i, :, :] * self.inner_weights for i in range(x.shape[0])], dim=0)

        # # 对edge_attr进行加权，元素对应相乘（pls模型的权重）
        # edge_attr = edge_attr.reshape(-1, 56, edge_attr.shape[1])
        # edge_attr = torch.cat([edge_attr[i, :, :] * self.outer_weights for i in range(edge_attr.shape[0])], dim=0)

        x1 = F.relu(self.conv1(x, edge_index, edge_attr))
        x1 = F.relu(self.conv2(x1, edge_index, edge_attr))
        x1 = self.conv_out(x1, edge_index, edge_attr)

        x2 = F.relu(self.fc1(x))
        x2 = F.relu(self.fc2(x2))

        # 合并
        # print(x1.shape, x2.shape)
        x = torch.cat([x1, x2], dim=1)
        # print(x.shape)
        # assert False
        # 展平
        x = x.reshape(-1, x.shape[1]*8)
        # print(x.shape)
        x = self.output(x)

        
        # # print(x.shape)
        # # print(batch, batch.shape)
        # # assert False
        # x = x.reshape(-1, 8)
        # # print(x.shape)
        # # 沿第二个维度平均池化
        # x = torch.mean(x, dim=1, keepdim=True)
        # # print(x.shape)
        # assert x.shape[1] == 1
        return x
    
    def loss(self, output, target):
        target = torch.tensor(np.array(target), dtype=torch.float32)
        # print(output.shape, target.shape)
        
        output = output.to('cuda')
        target = target.to('cuda')
        loss = F.mse_loss(output, target)
        # print(output, target)
        # print(loss)
        # assert False
        return loss


------------------------------------------

gat_loss_75

class GAT_MODEL(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, edge_features_dim, drop_rate=0.5, inner_weights=None, outer_weights=None):
        super(GAT_MODEL, self).__init__()
        self.inner_weights = inner_weights
        self.outer_weights = outer_weights

        # Linear层
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.output = nn.Linear(hidden_size*2*8, output_size)

        # GAT层
        self.conv1 = GATConv(
            -1, 
            hidden_size, 
            heads=8, 
            dropout=drop_rate,
            concat=True,
            negative_slope=0.4,
            add_self_loops=True,
            edge_dim=edge_features_dim,
            bias=True,
            residual=True)
        
        self.conv2 = GATConv(
            hidden_size*8, 
            hidden_size, 
            heads=8, 
            dropout=drop_rate,
            concat=True,
            negative_slope=0.4,
            add_self_loops=True,
            edge_dim=edge_features_dim,
            bias=True,
            residual=True)

        self.conv_out = GATConv(
            hidden_size*8, 
            hidden_size, 
            heads=1, 
            dropout=0.2,
            concat=False,
            negative_slope=0.4,
            add_self_loops=True,
            edge_dim=edge_features_dim,
            bias=True,
            residual=True)
        
    def forward(self, x, edge_index, edge_attr, edge_weight, batch):
        # 去除特殊值填充
        maskx = (x != 1000).float()
        x = x * maskx
        maske = (edge_attr != 1000).float()
        edge_attr = edge_attr * maske

        # # 对edge_attr进行加权，每一个权重值乘以一行特征(pls r2作为权重计算指标)
        # edge_attr = edge_attr * edge_weight.view(-1, 1)

        # # 对x进行加权，元素对应相乘（pls模型的权重）
        # x = x.reshape(-1, 8, x.shape[1])
        # x = torch.cat([x[i, :, :] * self.inner_weights for i in range(x.shape[0])], dim=0)

        # # 对edge_attr进行加权，元素对应相乘（pls模型的权重）
        # edge_attr = edge_attr.reshape(-1, 56, edge_attr.shape[1])
        # edge_attr = torch.cat([edge_attr[i, :, :] * self.outer_weights for i in range(edge_attr.shape[0])], dim=0)

        x1 = F.relu(self.conv1(x, edge_index, edge_attr))
        x1 = F.relu(self.conv2(x1, edge_index, edge_attr))
        x1 = self.conv_out(x1, edge_index, edge_attr)

        x2 = F.relu(self.fc1(x))
        x2 = F.relu(self.fc2(x2))

        # 合并
        # print(x1.shape, x2.shape)
        x = torch.cat([x1, x2], dim=1)
        # print(x.shape)
        # assert False
        # 展平
        x = x.reshape(-1, x.shape[1]*8)
        # print(x.shape)
        x = self.output(x)

        # # print(x.shape)
        # # print(batch, batch.shape)
        # # assert False
        # x = x.reshape(-1, 8)
        # # print(x.shape)
        # # 沿第二个维度平均池化
        # x = torch.mean(x, dim=1, keepdim=True)
        # # print(x.shape)
        # assert x.shape[1] == 1
        return x
    
    def loss(self, output, target):
        target = torch.tensor(np.array(target), dtype=torch.float32)
        # print(output.shape, target.shape)
        
        output = output.to('cuda')
        target = target.to('cuda')
        loss = F.mse_loss(output, target)
        # print(output, target)
        # print(loss)
        # assert False
        return loss
# 创建模型实例
model = GAT_MODEL(
    input_size=train_node_features.shape[2], 
    hidden_size=32, 
    output_size=1,
    edge_features_dim=train_edge_features.shape[2])

# 训练模型
best_model, train_loss_list, test_loss_list, r2_list, mse_list = train_gat(
    train_loader, test_loader, model, epochs=400, lr=0.001, weight_decay=1e-4, metrics=['mse', 'r2'], print_info=True, early_stop=40)


-----------------------------------

gat_loss_53

class GAT_MODEL(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, edge_features_dim, drop_rate=0.5, inner_weights=None, outer_weights=None):
        super(GAT_MODEL, self).__init__()
        self.inner_weights = inner_weights
        self.outer_weights = outer_weights

        # Linear层
        self.fc1_1 = nn.ModuleList([nn.Linear(input_size, hidden_size) for _ in range(8)])
        self.fc1_2 = nn.ModuleList([nn.Linear(hidden_size, hidden_size) for _ in range(8)])

        self.fc2_1 = nn.ModuleList([nn.Linear(edge_features_dim, hidden_size//2) for _ in range(56)])
        self.fc2_2 = nn.ModuleList([nn.Linear(hidden_size//2, hidden_size//2) for _ in range(56)])

        self.fc_merge = nn.Linear(32*8+16*56, hidden_size)

        self.output = nn.Linear(8+32, output_size)

        # GAT层
        self.conv1 = GATConv(
            -1, 
            hidden_size, 
            heads=8, 
            dropout=drop_rate,
            concat=True,
            negative_slope=0.4,
            add_self_loops=True,
            edge_dim=edge_features_dim,
            bias=True,
            residual=True)
        
        self.conv2 = GATConv(
            hidden_size*8, 
            hidden_size, 
            heads=8, 
            dropout=drop_rate,
            concat=True,
            negative_slope=0.4,
            add_self_loops=True,
            edge_dim=edge_features_dim,
            bias=True,
            residual=True)

        self.conv_out = GATConv(
            hidden_size*8, 
            output_size, 
            heads=1, 
            dropout=0.2,
            concat=False,
            negative_slope=0.4,
            add_self_loops=True,
            edge_dim=edge_features_dim,
            bias=True,
            residual=True)
        
    def forward(self, x, edge_index, edge_attr, edge_weight, batch):
        # 去除特殊值填充
        maskx = (x != 1000).float()
        x = x * maskx
        maske = (edge_attr != 1000).float()
        edge_attr = edge_attr * maske

        # # 对edge_attr进行加权，每一个权重值乘以一行特征(pls r2作为权重计算指标)
        # edge_attr = edge_attr * edge_weight.view(-1, 1)

        # # 对x进行加权，元素对应相乘（pls模型的权重）
        # x = x.reshape(-1, 8, x.shape[1])
        # x = torch.cat([x[i, :, :] * self.inner_weights for i in range(x.shape[0])], dim=0)

        # # 对edge_attr进行加权，元素对应相乘（pls模型的权重）
        # edge_attr = edge_attr.reshape(-1, 56, edge_attr.shape[1])
        # edge_attr = torch.cat([edge_attr[i, :, :] * self.outer_weights for i in range(edge_attr.shape[0])], dim=0)

        x1 = F.relu(self.conv1(x, edge_index, edge_attr))
        # print(x1.shape)
        x1 = F.relu(self.conv2(x1, edge_index, edge_attr))
        # print(x1.shape)
        x1 = self.conv_out(x1, edge_index, edge_attr)
        # print(x1.shape)
        x1 = x1.reshape(-1, 8)
        # print(x1.shape)
        # print('-----------------')

        # 节点特征子网络
        x2 = x.reshape(-1, 8, x.shape[1])
        # print(x2.shape)
        x2_1to8 = [x2[:, i, :] for i in range(x2.shape[1])] # 将8个节点特征分开
        # print(len(x2_1to8), x2_1to8[0].shape)

        # 使用self.fc1_1和self.fc1_2
        x2_1_1to8 = [F.relu(self.fc1_1[i](x2_1to8[i].squeeze())) for i in range(8)]
        # print(x2_1_1to8[0].shape)
        x2_2_1to8 = [F.relu(self.fc1_2[i](x2_1_1to8[i])) for i in range(8)]
        # print(x2_2_1to8[0].shape)
        x2 = torch.cat(x2_2_1to8, dim=1)
        # print(x2.shape)
        # print('-----------------')

        # assert False

        # 边特征子网络
        x3 = edge_attr.reshape(-1, 56, edge_attr.shape[1])
        x3_1to56 = [x3[:, i, :] for i in range(x3.shape[1])] # 将56个边特征分开
        # print(len(x3_1to56), x3_1to56[0].shape)
        # 使用self.fc2_1和self.fc2_2
        x3_1_1to56 = [F.relu(self.fc2_1[i](x3_1to56[i].squeeze())) for i in range(56)]
        # print(x3_1_1to56[0].shape)
        x3_2_1to56 = [F.relu(self.fc2_2[i](x3_1_1to56[i])) for i in range(56)]
        # print(x3_2_1to56[0].shape)
        x3 = torch.cat(x3_2_1to56, dim=1)
        # print(x3.shape)

        # assert False

        # 合并x2和x3
        # print('-'*20)
        x23 = torch.cat([x2, x3], dim=1)
        # print(x23.shape)
        x23 = F.relu(self.fc_merge(x23))
        # print(x23.shape)

        # assert False

        # 合并
        # print('-'*20)
        # print(x1.shape, x23.shape)
        x = torch.cat([x1, x23], dim=1)
        # print(x.shape)
        # assert False

        # 展平
        x = self.output(x)
        # print(x.shape)
        # # print(batch, batch.shape)
        # # assert False
        # x = x.reshape(-1, 8)
        # # print(x.shape)
        # # 沿第二个维度平均池化
        # x = torch.mean(x, dim=1, keepdim=True)
        # # print(x.shape)
        # assert x.shape[1] == 1
        # assert False

        return x
    
    def loss(self, output, target):
        target = torch.tensor(np.array(target), dtype=torch.float32)
        # print(output.shape, target.shape)
        
        output = output.to('cuda')
        target = target.to('cuda')
        loss = F.mse_loss(output, target)
        # print(output, target)
        # print(loss)
        # assert False
        return loss
# 创建模型实例
model = GAT_MODEL(
    input_size=train_node_features.shape[2], 
    hidden_size=32, 
    output_size=1,
    edge_features_dim=train_edge_features.shape[2])

# 训练模型
best_model, train_loss_list, test_loss_list, r2_list, mse_list = train_gat(
    train_loader, test_loader, model, epochs=400, lr=0.001, weight_decay=1e-4, metrics=['mse', 'r2'], print_info=True, early_stop=40)



--------------------------------------------

gat_loss_62


class GAT_MODEL(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, edge_features_dim, drop_rate=0.5, inner_weights=None, outer_weights=None):
        super(GAT_MODEL, self).__init__()
        self.inner_weights = inner_weights
        self.outer_weights = outer_weights

        # Linear层
        self.fc1_1 = nn.ModuleList([nn.Linear(input_size, hidden_size) for _ in range(8)])
        self.fc1_2 = nn.ModuleList([nn.Linear(hidden_size, hidden_size) for _ in range(8)])

        self.fc2_1 = nn.ModuleList([nn.Linear(edge_features_dim, hidden_size//4) for _ in range(56)])
        self.fc2_2 = nn.ModuleList([nn.Linear(hidden_size//4, hidden_size//8) for _ in range(56)])

        # GAT层
        self.conv1 = GATConv(
            -1, 
            hidden_size, 
            heads=8, 
            dropout=drop_rate,
            concat=True,
            negative_slope=0.4,
            add_self_loops=True,
            edge_dim=edge_features_dim,
            bias=True,
            residual=True)
        
        self.conv2 = GATConv(
            hidden_size*8, 
            hidden_size, 
            heads=8, 
            dropout=drop_rate,
            concat=True,
            negative_slope=0.4,
            add_self_loops=True,
            edge_dim=edge_features_dim,
            bias=True,
            residual=True)

        self.conv_out = GATConv(
            hidden_size*8, 
            output_size, 
            heads=1, 
            dropout=0.2,
            concat=False,
            negative_slope=0.4,
            add_self_loops=True,
            edge_dim=edge_features_dim,
            bias=True,
            residual=True)
        
        self.fc_merge = nn.Linear(480, hidden_size//2)

        self.output = nn.Linear(24+16, output_size)

    def forward(self, x, edge_index, edge_attr, edge_weight, batch):
        # 去除特殊值填充
        maskx = (x != 1000).float()
        x = x * maskx
        maske = (edge_attr != 1000).float()
        edge_attr = edge_attr * maske

        # # 对edge_attr进行加权，每一个权重值乘以一行特征(pls r2作为权重计算指标)
        # edge_attr = edge_attr * edge_weight.view(-1, 1)

        # # 对x进行加权，元素对应相乘（pls模型的权重）
        # x = x.reshape(-1, 8, x.shape[1])
        # x = torch.cat([x[i, :, :] * self.inner_weights for i in range(x.shape[0])], dim=0)

        # # 对edge_attr进行加权，元素对应相乘（pls模型的权重）
        # edge_attr = edge_attr.reshape(-1, 56, edge_attr.shape[1])
        # edge_attr = torch.cat([edge_attr[i, :, :] * self.outer_weights for i in range(edge_attr.shape[0])], dim=0)

        x1 = F.relu(self.conv1(x, edge_index, edge_attr))
        # print(x1.shape)
        x1 = F.relu(self.conv2(x1, edge_index, edge_attr))
        # print(x1.shape)
        x1 = self.conv_out(x1, edge_index, edge_attr)
        # print(x1.shape)
        x1 = x1.reshape(-1, 8)
        # print(x1.shape)
        # 沿第二个维度平均池化
        x1_1 = torch.mean(x, dim=1, keepdim=True)
        # 沿第二个维度最大池化
        x1_2 = torch.max(x, dim=1, keepdim=True)[0]
        # 沿第二个维度最小池化
        x1_3 = torch.min(x, dim=1, keepdim=True)[0]
        # 合并
        x1 = torch.cat([x1_1, x1_2, x1_3], dim=1)
        # print(x1.shape)
        x1 = x1.reshape(-1, 3*8)
        # print(x1.shape)
        # print('-----------------')
        # assert False

        # 节点特征子网络
        x2 = x.reshape(-1, 8, x.shape[1])
        # print(x2.shape)
        x2_1to8 = [x2[:, i, :] for i in range(x2.shape[1])] # 将8个节点特征分开
        # print(len(x2_1to8), x2_1to8[0].shape)

        # 使用self.fc1_1和self.fc1_2
        x2_1_1to8 = [F.relu(self.fc1_1[i](x2_1to8[i].squeeze())) for i in range(8)]
        # print(x2_1_1to8[0].shape)
        x2_2_1to8 = [F.relu(self.fc1_2[i](x2_1_1to8[i])) for i in range(8)]
        # print(x2_2_1to8[0].shape)
        x2 = torch.cat(x2_2_1to8, dim=1)
        # print(x2.shape)
        # print('-----------------')

        # assert False

        # 边特征子网络
        x3 = edge_attr.reshape(-1, 56, edge_attr.shape[1])
        x3_1to56 = [x3[:, i, :] for i in range(x3.shape[1])] # 将56个边特征分开
        # print(len(x3_1to56), x3_1to56[0].shape)
        # 使用self.fc2_1和self.fc2_2
        x3_1_1to56 = [F.relu(self.fc2_1[i](x3_1to56[i].squeeze())) for i in range(56)]
        # print(x3_1_1to56[0].shape)
        x3_2_1to56 = [F.relu(self.fc2_2[i](x3_1_1to56[i])) for i in range(56)]
        # print(x3_2_1to56[0].shape)
        x3 = torch.cat(x3_2_1to56, dim=1)
        # print(x3.shape)
        # print('-'*20)
        # assert False

        # 合并x2和x3
        
        x23 = torch.cat([x2, x3], dim=1)
        # print(x23.shape)
        x23 = F.relu(self.fc_merge(x23))
        # print(x23.shape)
        # print('-'*20)
        # assert False

        # 合并
        # print('-'*20)
        # print(x1.shape, x23.shape)
        x = F.relu(torch.cat([x1, x23], dim=1))
        # print(x.shape)
        # assert False

        # 展平
        x = self.output(x)
        # print(x.shape)
        # # print(batch, batch.shape)
        # # assert False
        # # print(x.shape)
        # assert x.shape[1] == 1
        # assert False

        return x
    
    def loss(self, output, target):
        target = torch.tensor(np.array(target), dtype=torch.float32)
        # print(output.shape, target.shape)
        
        output = output.to('cuda')
        target = target.to('cuda')
        loss = F.mse_loss(output, target)
        # print(output, target)
        # print(loss)
        # assert False
        return loss
# 创建模型实例
model = GAT_MODEL(
    input_size=train_node_features.shape[2], 
    hidden_size=32, 
    output_size=1,
    edge_features_dim=train_edge_features.shape[2])

# 训练模型
best_model, train_loss_list, test_loss_list, r2_list, mse_list = train_gat(
    train_loader, test_loader, model, epochs=400, lr=0.001, weight_decay=1e-4, metrics=['mse', 'r2'], print_info=True, early_stop=30)



------------------------------------------------

gat_loss_85


class GAT_MODEL(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, edge_features_dim, drop_rate=0.5, inner_weights=None, outer_weights=None):
        super(GAT_MODEL, self).__init__()
        self.inner_weights = inner_weights
        self.outer_weights = outer_weights

        # Linear层
        self.fc1 = nn.Linear(hidden_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)

        # GAT层
        self.conv1 = nn.ModuleList([GATConv(
            -1, 
            hidden_size, 
            heads=8, 
            dropout=drop_rate,
            concat=True,
            negative_slope=0.2,
            add_self_loops=True,
            edge_dim=edge_features_dim,
            bias=True,
            residual=True) for _ in range(4)])
      
        self.conv_out = GATConv(
            hidden_size*8, 
            output_size, 
            heads=1, 
            dropout=0.2,
            concat=False,
            negative_slope=0.2,
            add_self_loops=True,
            edge_dim=edge_features_dim,
            bias=True,
            residual=True)
        
    def forward(self, x, edge_index, edge_attr, edge_weight, batch):
        # 去除特殊值填充
        maskx = (x != 1000).float()
        x = x * maskx
        maske = (edge_attr != 1000).float()
        edge_attr = edge_attr * maske

        # # 对edge_attr进行加权，每一个权重值乘以一行特征(pls r2作为权重计算指标)
        # edge_attr = edge_attr * edge_weight.view(-1, 1)

        # # 对x进行加权，元素对应相乘（pls模型的权重）
        # x = x.reshape(-1, 8, x.shape[1])
        # x = torch.cat([x[i, :, :] * self.inner_weights for i in range(x.shape[0])], dim=0)

        # # 对edge_attr进行加权，元素对应相乘（pls模型的权重）
        # edge_attr = edge_attr.reshape(-1, 56, edge_attr.shape[1])
        # edge_attr = torch.cat([edge_attr[i, :, :] * self.outer_weights for i in range(edge_attr.shape[0])], dim=0)

        # GAT层
        for conv in self.conv1:
            x = conv(x, edge_index, edge_attr)
            x = F.elu(x)
            x = F.dropout(x, p=0.5, training=self.training)

        x = self.conv_out(x, edge_index, edge_attr)
        
        # print(x.shape)
        # print(batch, batch.shape)
        # assert False
        x = x.reshape(-1, 8)
        # print(x.shape)
        # 沿第二个维度平均池化
        x = torch.mean(x, dim=1, keepdim=True)
        # print(x.shape)
        assert x.shape[1] == 1
        return x
    
    def loss(self, output, target):
        target = torch.tensor(np.array(target), dtype=torch.float32)
        # print(output.shape, target.shape)
        
        output = output.to('cuda')
        target = target.to('cuda')
        loss = F.mse_loss(output, target)
        # print(output, target)
        # print(loss)
        # assert False
        return loss
    
# 创建模型实例
model = GAT_MODEL(
    input_size=train_node_features.shape[2], 
    hidden_size=32, 
    output_size=1,
    edge_features_dim=train_edge_features.shape[2])

# 训练模型
best_model, train_loss_list, test_loss_list, r2_list, mse_list = train_gat(
    train_loader, test_loader, model, epochs=400, lr=0.001, weight_decay=1e-4, metrics=['mse', 'r2'], print_info=True, early_stop=30)


----------------------------------------------


92

Best R²: 0.6259, Best MSE: 4461.4286

class GAT_MODEL(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, edge_features_dim, drop_rate=0.5, inner_weights=None, outer_weights=None):
        super(GAT_MODEL, self).__init__()
        self.inner_weights = inner_weights
        self.outer_weights = outer_weights

        # Linear层
        self.fc1 = nn.Linear(hidden_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)

        # GAT层
        self.conv1 = nn.ModuleList([GATConv(
            -1, 
            hidden_size, 
            heads=8, 
            dropout=drop_rate,
            concat=True,
            negative_slope=0.2,
            add_self_loops=True,
            edge_dim=edge_features_dim,
            bias=True,
            residual=True) for _ in range(6)])
      
        self.conv_out = GATConv(
            hidden_size*8, 
            output_size, 
            heads=1, 
            dropout=0.2,
            concat=False,
            negative_slope=0.2,
            add_self_loops=True,
            edge_dim=edge_features_dim,
            bias=True,
            residual=True)
        
    def forward(self, x, edge_index, edge_attr, edge_weight, batch):
        # 去除特殊值填充
        maskx = (x != 1000).float()
        x = x * maskx
        maske = (edge_attr != 1000).float()
        edge_attr = edge_attr * maske

        # # 对edge_attr进行加权，每一个权重值乘以一行特征(pls r2作为权重计算指标)
        # edge_attr = edge_attr * edge_weight.view(-1, 1)

        # # 对x进行加权，元素对应相乘（pls模型的权重）
        # x = x.reshape(-1, 8, x.shape[1])
        # x = torch.cat([x[i, :, :] * self.inner_weights for i in range(x.shape[0])], dim=0)

        # # 对edge_attr进行加权，元素对应相乘（pls模型的权重）
        # edge_attr = edge_attr.reshape(-1, 56, edge_attr.shape[1])
        # edge_attr = torch.cat([edge_attr[i, :, :] * self.outer_weights for i in range(edge_attr.shape[0])], dim=0)

        # GAT层
        for conv in self.conv1:
            x = conv(x, edge_index, edge_attr)
            x = F.elu(x)
            x = F.dropout(x, p=0.5, training=self.training)

        x = self.conv_out(x, edge_index, edge_attr)
        
        # print(x.shape)
        # print(batch, batch.shape)
        # assert False
        x = x.reshape(-1, 8)
        # print(x.shape)
        # 沿第二个维度平均池化
        x = torch.mean(x, dim=1, keepdim=True)
        # print(x.shape)
        assert x.shape[1] == 1
        return x
    
    def loss(self, output, target):
        target = torch.tensor(np.array(target), dtype=torch.float32)
        # print(output.shape, target.shape)
        
        output = output.to('cuda')
        target = target.to('cuda')
        loss = F.mse_loss(output, target)
        # print(output, target)
        # print(loss)
        # assert False
        return loss
    
# 创建模型实例
model = GAT_MODEL(
    input_size=train_node_features.shape[2], 
    hidden_size=32, 
    output_size=1,
    edge_features_dim=train_edge_features.shape[2])

# 训练模型
best_model, train_loss_list, test_loss_list, r2_list, mse_list = train_gat(
    train_loader, test_loader, model, epochs=400, lr=0.001, weight_decay=1e-4, metrics=['mse', 'r2'], print_info=True, early_stop=30)


------------------------------------------------

105

R²: 0.5183, MSE: 5745.2325

torch.cuda.empty_cache()

class GAT_MODEL(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, edge_features_dim, drop_rate=0.2, inner_weights=None, outer_weights=None):
        super(GAT_MODEL, self).__init__()
        self.inner_weights = inner_weights
        self.outer_weights = outer_weights

        # Linear层
        self.fc1 = nn.Linear(hidden_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)

        # GAT层
        self.conv1 = nn.ModuleList([GATConv(
            -1, 
            hidden_size, 
            heads=8, 
            dropout=drop_rate,
            concat=True,
            negative_slope=0.2,
            add_self_loops=True,
            edge_dim=edge_features_dim,
            bias=True,
            residual=True) for _ in range(6)])
      
        self.conv_out = GATConv(
            hidden_size*8, 
            output_size, 
            heads=1, 
            dropout=drop_rate,
            concat=False,
            negative_slope=0.2,
            add_self_loops=True,
            edge_dim=edge_features_dim,
            bias=True,
            residual=True)
        
    def forward(self, x, edge_index, edge_attr, edge_weight, batch):
        # 去除特殊值填充
        maskx = (x != 1000).float()
        x = x * maskx
        maske = (edge_attr != 1000).float()
        edge_attr = edge_attr * maske

        # # 对edge_attr进行加权，每一个权重值乘以一行特征(pls r2作为权重计算指标)
        # edge_attr = edge_attr * edge_weight.view(-1, 1)

        # # 对x进行加权，元素对应相乘（pls模型的权重）
        # x = x.reshape(-1, 8, x.shape[1])
        # x = torch.cat([x[i, :, :] * self.inner_weights for i in range(x.shape[0])], dim=0)

        # # 对edge_attr进行加权，元素对应相乘（pls模型的权重）
        # edge_attr = edge_attr.reshape(-1, 56, edge_attr.shape[1])
        # edge_attr = torch.cat([edge_attr[i, :, :] * self.outer_weights for i in range(edge_attr.shape[0])], dim=0)

        # GAT层
        for conv in self.conv1:
            x = conv(x, edge_index, edge_attr)
            x = F.elu(x)
            x = F.dropout(x, p=0.5, training=self.training)

        x = self.conv_out(x, edge_index, edge_attr)
        
        # print(x.shape)
        # print(batch, batch.shape)
        # assert False
        x = x.reshape(-1, 8)
        # print(x.shape)
        # 沿第二个维度平均池化
        x = torch.mean(x, dim=1, keepdim=True)
        # print(x.shape)
        assert x.shape[1] == 1
        return x
    
    def loss(self, output, target):
        target = torch.tensor(np.array(target), dtype=torch.float32)
        # print(output.shape, target.shape)
        
        output = output.to('cuda')
        target = target.to('cuda')
        loss = F.mse_loss(output, target)
        # print(output, target)
        # print(loss)
        # assert False
        return loss
    
# 创建模型实例
model = GAT_MODEL(
    input_size=train_node_features.shape[2], 
    hidden_size=32, 
    output_size=1,
    edge_features_dim=train_edge_features.shape[2])

# 训练模型
best_model, train_loss_list, test_loss_list, r2_list, mse_list = train_gat(
    train_loader, test_loader, model, epochs=400, lr=0.001, weight_decay=1e-4, metrics=['mse', 'r2'], print_info=True, early_stop=40)


-----------------------------------------------

80
R²: 0.4463, MSE: 6604.1347

80_1 better

torch.cuda.empty_cache()

class GAT_MODEL(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, edge_features_dim, drop_rate=0.2, inner_weights=None, outer_weights=None):
        super(GAT_MODEL, self).__init__()
        self.inner_weights = inner_weights
        self.outer_weights = outer_weights

        # Linear层
        self.fc1 = nn.Linear(hidden_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)

        # GAT层
        self.conv1 = nn.ModuleList([GATConv(
            -1, 
            hidden_size, 
            heads=8, 
            dropout=drop_rate,
            concat=True,
            negative_slope=0.2,
            add_self_loops=True,
            edge_dim=edge_features_dim,
            bias=True,
            residual=True) for _ in range(6)])
      
        self.conv_out = GATConv(
            hidden_size*8, 
            output_size, 
            heads=1, 
            dropout=drop_rate,
            concat=False,
            negative_slope=0.2,
            add_self_loops=True,
            edge_dim=edge_features_dim,
            bias=True,
            residual=True)
        
    def forward(self, x, edge_index, edge_attr, edge_weight, batch):
        # 去除特殊值填充
        maskx = (x != 1000).float()
        x = x * maskx
        maske = (edge_attr != 1000).float()
        edge_attr = edge_attr * maske

        # # 对edge_attr进行加权，每一个权重值乘以一行特征(pls r2作为权重计算指标)
        # edge_attr = edge_attr * edge_weight.view(-1, 1)

        # # 对x进行加权，元素对应相乘（pls模型的权重）
        # x = x.reshape(-1, 8, x.shape[1])
        # x = torch.cat([x[i, :, :] * self.inner_weights for i in range(x.shape[0])], dim=0)

        # # 对edge_attr进行加权，元素对应相乘（pls模型的权重）
        # edge_attr = edge_attr.reshape(-1, 56, edge_attr.shape[1])
        # edge_attr = torch.cat([edge_attr[i, :, :] * self.outer_weights for i in range(edge_attr.shape[0])], dim=0)

        # GAT层
        for conv in self.conv1:
            x = conv(x, edge_index, edge_attr)
            x = F.elu(x)
            x = F.dropout(x, p=0.5, training=self.training)

        x = self.conv_out(x, edge_index, edge_attr)
        
        # print(x.shape)
        # print(batch, batch.shape)
        # assert False
        x = x.reshape(-1, 8)
        # print(x.shape)
        # 沿第二个维度平均池化
        x = torch.mean(x, dim=1, keepdim=True)
        # print(x.shape)
        assert x.shape[1] == 1
        return x
    
    def loss(self, output, target):
        target = torch.tensor(np.array(target), dtype=torch.float32)
        # print(output.shape, target.shape)
        
        output = output.to('cuda')
        target = target.to('cuda')
        loss = F.mse_loss(output, target)
        # print(output, target)
        # print(loss)
        # assert False
        return loss
    
# 创建模型实例
model = GAT_MODEL(
    input_size=train_node_features.shape[2], 
    hidden_size=64, 
    output_size=1,
    edge_features_dim=train_edge_features.shape[2])

# 训练模型
best_model, train_loss_list, test_loss_list, r2_list, mse_list = train_gat(
    train_loader, test_loader, model, epochs=400, lr=0.001, weight_decay=1e-4, metrics=['mse', 'r2'], print_info=True, early_stop=40)


----------------------------------------------


83

Best R²: 0.5175, Best MSE: 5755.4665

class GAT_MODEL(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, edge_features_dim, drop_rate=0.5, inner_weights=None, outer_weights=None):
        super(GAT_MODEL, self).__init__()
        self.inner_weights = inner_weights
        self.outer_weights = outer_weights

        # Linear层
        self.fc1 = nn.Linear(hidden_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)

        # GAT层
        self.conv1 = nn.ModuleList([GATConv(
            -1, 
            hidden_size, 
            heads=8, 
            dropout=drop_rate,
            concat=True,
            negative_slope=0.2,
            add_self_loops=True,
            edge_dim=edge_features_dim,
            bias=True,
            residual=True) for _ in range(6)])
      
        self.conv_out = GATConv(
            hidden_size*8, 
            output_size, 
            heads=1, 
            dropout=0.2,
            concat=False,
            negative_slope=0.2,
            add_self_loops=True,
            edge_dim=edge_features_dim,
            bias=True,
            residual=True)
        
    def forward(self, x, edge_index, edge_attr, edge_weight, batch):
        # 去除特殊值填充
        maskx = (x != 1000).float()
        x = x * maskx
        maske = (edge_attr != 1000).float()
        edge_attr = edge_attr * maske

        # # 对edge_attr进行加权，每一个权重值乘以一行特征(pls r2作为权重计算指标)
        # edge_attr = edge_attr * edge_weight.view(-1, 1)

        # # 对x进行加权，元素对应相乘（pls模型的权重）
        # x = x.reshape(-1, 8, x.shape[1])
        # x = torch.cat([x[i, :, :] * self.inner_weights for i in range(x.shape[0])], dim=0)

        # # 对edge_attr进行加权，元素对应相乘（pls模型的权重）
        # edge_attr = edge_attr.reshape(-1, 56, edge_attr.shape[1])
        # edge_attr = torch.cat([edge_attr[i, :, :] * self.outer_weights for i in range(edge_attr.shape[0])], dim=0)

        # GAT层
        for conv in self.conv1:
            x = conv(x, edge_index, edge_attr)
            x = F.elu(x)
            x = F.dropout(x, p=0.5, training=self.training)

        x = self.conv_out(x, edge_index, edge_attr)
        
        # print(x.shape)
        # print(batch, batch.shape)
        # assert False
        x = x.reshape(-1, 8)
        # print(x.shape)
        # 沿第二个维度平均池化
        x = torch.mean(x, dim=1, keepdim=True)
        # print(x.shape)
        assert x.shape[1] == 1
        return x
    
    def loss(self, output, target):
        target = torch.tensor(np.array(target), dtype=torch.float32)
        # print(output.shape, target.shape)
        
        output = output.to('cuda')
        target = target.to('cuda')
        loss = F.mse_loss(output, target)
        # print(output, target)
        # print(loss)
        # assert False
        return loss
    
# 创建模型实例
model = GAT_MODEL(
    input_size=train_node_features.shape[2], 
    hidden_size=32, 
    output_size=1,
    edge_features_dim=train_edge_features.shape[2])

# 训练模型
best_model, train_loss_list, test_loss_list, r2_list, mse_list = train_gat(
    train_loader, test_loader, model, epochs=400, lr=0.001, weight_decay=1e-4, metrics=['mse', 'r2'], print_info=True, early_stop=70)


---------------------------------------------------

133

Best R²: 0.5183, Best MSE: 5745.2319

class GAT_MODEL(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, edge_features_dim, drop_rate=0.5, inner_weights=None, outer_weights=None):
        super(GAT_MODEL, self).__init__()
        self.inner_weights = inner_weights
        self.outer_weights = outer_weights

        # Linear层
        self.fc1 = nn.Linear(hidden_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)

        # GAT层
        self.conv1 = nn.ModuleList([GATConv(
            -1, 
            hidden_size, 
            heads=16, 
            dropout=drop_rate,
            concat=True,
            negative_slope=0.2,
            add_self_loops=True,
            edge_dim=edge_features_dim,
            bias=True,
            residual=True) for _ in range(6)])
      
        self.conv_out = GATConv(
            hidden_size*16, 
            output_size, 
            heads=1, 
            dropout=0.2,
            concat=False,
            negative_slope=0.2,
            add_self_loops=True,
            edge_dim=edge_features_dim,
            bias=True,
            residual=True)
        
    def forward(self, x, edge_index, edge_attr, edge_weight, batch):
        # 去除特殊值填充
        maskx = (x != 1000).float()
        x = x * maskx
        maske = (edge_attr != 1000).float()
        edge_attr = edge_attr * maske

        # # 对edge_attr进行加权，每一个权重值乘以一行特征(pls r2作为权重计算指标)
        # edge_attr = edge_attr * edge_weight.view(-1, 1)

        # # 对x进行加权，元素对应相乘（pls模型的权重）
        # x = x.reshape(-1, 8, x.shape[1])
        # x = torch.cat([x[i, :, :] * self.inner_weights for i in range(x.shape[0])], dim=0)

        # # 对edge_attr进行加权，元素对应相乘（pls模型的权重）
        # edge_attr = edge_attr.reshape(-1, 56, edge_attr.shape[1])
        # edge_attr = torch.cat([edge_attr[i, :, :] * self.outer_weights for i in range(edge_attr.shape[0])], dim=0)

        # GAT层
        for conv in self.conv1:
            x = conv(x, edge_index, edge_attr)
            x = F.elu(x)
            x = F.dropout(x, p=0.5, training=self.training)

        x = self.conv_out(x, edge_index, edge_attr)
        
        # print(x.shape)
        # print(batch, batch.shape)
        # assert False
        x = x.reshape(-1, 8)
        # print(x.shape)
        # 沿第二个维度平均池化
        x = torch.mean(x, dim=1, keepdim=True)
        # print(x.shape)
        assert x.shape[1] == 1
        return x
    
    def loss(self, output, target):
        target = torch.tensor(np.array(target), dtype=torch.float32)
        # print(output.shape, target.shape)
        
        output = output.to('cuda')
        target = target.to('cuda')
        loss = F.mse_loss(output, target)
        # print(output, target)
        # print(loss)
        # assert False
        return loss
    
# 创建模型实例
model = GAT_MODEL(
    input_size=train_node_features.shape[2], 
    hidden_size=32, 
    output_size=1,
    edge_features_dim=train_edge_features.shape[2])

# 训练模型
best_model, train_loss_list, test_loss_list, r2_list, mse_list = train_gat(
    train_loader, test_loader, model, epochs=400, lr=0.001, weight_decay=1e-4, metrics=['mse', 'r2'], print_info=True, early_stop=70)


--------------------------------------


71

Best R²: 0.5670, Best MSE: 5165.0418

class GAT_MODEL(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, edge_features_dim, drop_rate=0.5, inner_weights=None, outer_weights=None):
        super(GAT_MODEL, self).__init__()
        self.inner_weights = inner_weights
        self.outer_weights = outer_weights

        # Linear层
        self.fc1 = nn.Linear(hidden_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)

        # GAT层
        self.conv1 = nn.ModuleList([GATConv(
            -1, 
            hidden_size, 
            heads=8, 
            dropout=drop_rate,
            concat=True,
            negative_slope=0.2,
            add_self_loops=True,
            edge_dim=edge_features_dim,
            bias=True,
            residual=True) for _ in range(6)])
      
        self.conv_out = GATConv(
            hidden_size*8, 
            output_size, 
            heads=1, 
            dropout=0.2,
            concat=False,
            negative_slope=0.2,
            add_self_loops=True,
            edge_dim=edge_features_dim,
            bias=True,
            residual=True)
        
    def forward(self, x, edge_index, edge_attr, edge_weight, batch):
        # 去除特殊值填充
        maskx = (x != 1000).float()
        x = x * maskx
        maske = (edge_attr != 1000).float()
        edge_attr = edge_attr * maske

        # # 对edge_attr进行加权，每一个权重值乘以一行特征(pls r2作为权重计算指标)
        # edge_attr = edge_attr * edge_weight.view(-1, 1)

        # # 对x进行加权，元素对应相乘（pls模型的权重）
        # x = x.reshape(-1, 8, x.shape[1])
        # x = torch.cat([x[i, :, :] * self.inner_weights for i in range(x.shape[0])], dim=0)

        # # 对edge_attr进行加权，元素对应相乘（pls模型的权重）
        # edge_attr = edge_attr.reshape(-1, 56, edge_attr.shape[1])
        # edge_attr = torch.cat([edge_attr[i, :, :] * self.outer_weights for i in range(edge_attr.shape[0])], dim=0)

        # GAT层
        for conv in self.conv1:
            x = conv(x, edge_index, edge_attr)
            x = F.elu(x)
            x = F.dropout(x, p=0.2, training=self.training)

        x = self.conv_out(x, edge_index, edge_attr)
        
        # print(x.shape)
        # print(batch, batch.shape)
        # assert False
        x = x.reshape(-1, 8)
        # print(x.shape)
        # 沿第二个维度平均池化
        x = torch.mean(x, dim=1, keepdim=True)
        # print(x.shape)
        assert x.shape[1] == 1
        return x
    
    def loss(self, output, target):
        target = torch.tensor(np.array(target), dtype=torch.float32)
        # print(output.shape, target.shape)
        
        output = output.to('cuda')
        target = target.to('cuda')
        loss = F.mse_loss(output, target)
        # print(output, target)
        # print(loss)
        # assert False
        return loss
    
# 创建模型实例
model = GAT_MODEL(
    input_size=train_node_features.shape[2], 
    hidden_size=32, 
    output_size=1,
    edge_features_dim=train_edge_features.shape[2])

# 训练模型
best_model, train_loss_list, test_loss_list, r2_list, mse_list = train_gat(
    train_loader, test_loader, model, epochs=400, lr=0.001, weight_decay=1e-4, metrics=['mse', 'r2'], print_info=True, early_stop=70)

# 保存模型
# 是否存在同名文件
if os.path.exists('gat_model.pth'):
    # 提示
    print("Model file already exists.")
    # 是否覆盖
    overwrite = input("Overwrite? (y/n): ")
    if overwrite.lower() == 'y':
        model_save_path = os.path.join('.', 'gat_model.pth')
        torch.save(best_model, model_save_path)
else:
    model_save_path = os.path.join('.', 'gat_model.pth')
    torch.save(best_model, model_save_path)