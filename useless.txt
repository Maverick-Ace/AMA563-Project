# 定义一个1D卷积的基本块
class ResidualBlock1D(nn.Module):
    # 扩展因子
    expansion = 1

    # 初始化函数，定义卷积层、批归一化层和ReLU激活函数
    def __init__(self, in_channels, out_channels, stride=1, downsample=None):
        super(ResidualBlock1D, self).__init__()
        # 第一个卷积层
        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)
        # 第一个批归一化层
        self.bn1 = nn.BatchNorm1d(out_channels)
        # 第二个卷积层
        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)
        # 第二个批归一化层
        self.bn2 = nn.BatchNorm1d(out_channels)
        # 下采样层
        self.downsample = downsample
        # ReLU激活函数
        self.relu = nn.ReLU(inplace=True)

    # 前向传播函数
    def forward(self, x):
        # 保存输入数据
        identity = x

        # 第一个卷积层、批归一化层和ReLU激活函数
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        # 第二个卷积层和批归一化层
        out = self.conv2(out)
        out = self.bn2(out)

        # 如果存在下采样层，则对输入数据进行下采样
        if self.downsample is not None:
            identity = self.downsample(x)

        # 将输入数据与卷积层输出相加，并通过ReLU激活函数
        out += identity
        out = self.relu(out)

        # 返回输出数据
        return out
    

class GAT_MODEL(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, edge_features_dim, drop_rate=0.5, inner_weights=None, outer_weights=None):
        super(GAT_MODEL, self).__init__()
        self.inner_weights = inner_weights
        self.outer_weights = outer_weights

        # resnet层
        self.resnet1_1 = nn.ModuleList([ResidualBlock1D(8, 8) for _ in range(3)])
        self.resnet1_2 = nn.ModuleList([ResidualBlock1D(8, 8) for _ in range(3)])

        self.resnet2_1 = nn.ModuleList([ResidualBlock1D(56, 56) for _ in range(3)])
        self.resnet2_2 = nn.ModuleList([ResidualBlock1D(56, 56) for _ in range(3)])

        # 池化层
        self.pool1 = nn.AdaptiveAvgPool1d(512)
        self.pool2 = nn.AdaptiveAvgPool1d(56)

        # GAT层
        self.conv1 = GATConv(
            -1, 
            hidden_size, 
            heads=8, 
            dropout=drop_rate,
            concat=True,
            negative_slope=0.4,
            add_self_loops=True,
            edge_dim=edge_features_dim,
            bias=True,
            residual=True)
        
        self.conv2 = GATConv(
            hidden_size*8, 
            hidden_size, 
            heads=8, 
            dropout=drop_rate,
            concat=True,
            negative_slope=0.4,
            add_self_loops=True,
            edge_dim=edge_features_dim,
            bias=True,
            residual=True)

        self.conv_out = GATConv(
            hidden_size*8, 
            output_size, 
            heads=1, 
            dropout=0.2,
            concat=False,
            negative_slope=0.4,
            add_self_loops=True,
            edge_dim=edge_features_dim,
            bias=True,
            residual=True)
        
        self.fc_merge = nn.Linear(3584, hidden_size)

        self.output = nn.Linear(24+32, output_size)

    def forward(self, x, edge_index, edge_attr, edge_weight, batch):
        # 去除特殊值填充
        maskx = (x != 1000).float()
        x = x * maskx
        maske = (edge_attr != 1000).float()
        edge_attr = edge_attr * maske

        # # 对edge_attr进行加权，每一个权重值乘以一行特征(pls r2作为权重计算指标)
        # edge_attr = edge_attr * edge_weight.view(-1, 1)

        # # 对x进行加权，元素对应相乘（pls模型的权重）
        # x = x.reshape(-1, 8, x.shape[1])
        # x = torch.cat([x[i, :, :] * self.inner_weights for i in range(x.shape[0])], dim=0)

        # # 对edge_attr进行加权，元素对应相乘（pls模型的权重）
        # edge_attr = edge_attr.reshape(-1, 56, edge_attr.shape[1])
        # edge_attr = torch.cat([edge_attr[i, :, :] * self.outer_weights for i in range(edge_attr.shape[0])], dim=0)

        x1 = F.relu(self.conv1(x, edge_index, edge_attr))
        # print(x1.shape)
        x1 = F.relu(self.conv2(x1, edge_index, edge_attr))
        # print(x1.shape)
        x1 = self.conv_out(x1, edge_index, edge_attr)
        # print(x1.shape)
        x1 = x1.reshape(-1, 8)
        # print(x1.shape)
        # 沿第二个维度平均池化
        x1_1 = torch.mean(x, dim=1, keepdim=True)
        # 沿第二个维度最大池化
        x1_2 = torch.max(x, dim=1, keepdim=True)[0]
        # 沿第二个维度最小池化
        x1_3 = torch.min(x, dim=1, keepdim=True)[0]
        # 合并
        x1 = torch.cat([x1_1, x1_2, x1_3], dim=1)
        # print(x1.shape)
        x1 = x1.reshape(-1, 3*8)
        # print(x1.shape)
        # print('-----------------')
        # assert False

        # 节点特征resnet blocks
        x2 = x.reshape(-1, 8, x.shape[1])
        # print(x2.shape)
        # 通过resnet1
        for block in self.resnet1_1:
            x2 = block(x2)
        # 池化层
        x2 = self.pool1(x2)
        for block in self.resnet1_2:
            x2 = block(x2)
        # 池化层
        x2 = self.pool2(x2)
        # print(x2.shape)

        # 边特征resnet blocks
        x3 = edge_attr.reshape(-1, 56, edge_attr.shape[1])
        # print(x3.shape)
        # 通过resnet2
        for block in self.resnet2_1:
            x3 = block(x3)
        # 池化层
        x3 = self.pool1(x3)
        for block in self.resnet2_2:
            x3 = block(x3)
        # 池化层
        x3 = self.pool2(x3)
        # print(x3.shape)
        # print('-'*20)

        # 合并x2, x3
        x23 = torch.cat([x2, x3], dim=1)
        # print(x23.shape)
        # 将后两个维度合并
        x23 = x23.reshape(x23.shape[0], -1)
        # print(x23.shape)
        x23 = self.fc_merge(x23)
        # print(x23.shape)
        # print('-'*20)

        # 合并x1, x23
        # print(x1.shape, x23.shape)
        x = F.relu(torch.cat([x1, x23], dim=1))
        # print(x.shape)
        x = self.output(x)
        # print(x.shape)
        # assert False

        return x
    
    def loss(self, output, target):
        target = torch.tensor(np.array(target), dtype=torch.float32)
        # print(output.shape, target.shape)
        
        output = output.to('cuda')
        target = target.to('cuda')
        loss = F.mse_loss(output, target)
        # print(output, target)
        # print(loss)
        # assert False
        return loss
# 创建模型实例
model = GAT_MODEL(
    input_size=train_node_features.shape[2], 
    hidden_size=32, 
    output_size=1,
    edge_features_dim=train_edge_features.shape[2])

# 训练模型
best_model, train_loss_list, test_loss_list, r2_list, mse_list = train_gat(
    train_loader, test_loader, model, epochs=400, lr=0.001, weight_decay=1e-4, metrics=['mse', 'r2'], print_info=True, early_stop=30)

# 保存模型
save_model = input("Save model? (y/n): ")
if save_model.lower() == 'y':
    # 是否存在同名文件
    if os.path.exists('gat_model.pth'):
        # 提示
        print("Model file already exists.")
        # 是否覆盖
        overwrite = input("Overwrite? (y/n): ")
        if overwrite.lower() == 'y':
            model_save_path = os.path.join('.', 'gat_model.pth')
            torch.save(best_model, model_save_path)
    else:
        model_save_path = os.path.join('.', 'gat_model.pth')
        torch.save(best_model, model_save_path)

---------------------------------------------------